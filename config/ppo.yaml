model_name: llm-slice/babylm-gpt2-small-90M-seed41
revision_name: chck_900M
learning_rate: 1e-5
log_with: wandb
hf_org" : llm-slice



# Trainer Settings
num_epochs: 1
batch_size: 1
token_limit: 2000
# checkpoint_interval: 100000
save_base_dir: saved_models/


#query settings (range for sampling length of student input)
query_min_length: 64
query_max_length: 128


data_path: data/ppo/

# Output generation
generation_kwargs:
  min_new_tokens: 64
  max_new_tokens: 128
  top_k: 80
  top_p: 0.9
  do_sample: true


model_name: llm-slice/babylm-gpt2-small-90M-seed41
revision_name: chck_900M
learning_rate: 1e-6
log_with: wandb
hf_org" : llm-slice



# Trainer Settings
num_epochs: 1
batch_size: 220
token_limit: 1600000
# checkpoint_interval: 200000
save_base_dir: saved_models/


#query settings (range for sampling length of student input)
query_min_length: 1
query_max_length: 8


data_path: data/ppo/

# Output generation
generation_kwargs:
  min_length: -1
  max_new_tokens: 90
  top_k: 0
  top_p: 1
  do_sample: true
  num_beams: 1


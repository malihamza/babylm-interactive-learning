model_name: gpt2
learning_rate: 1e-5
log_with: wandb
hf_org" : llm-slice



# Trainer Settings
num_epochs: 1
batch_size: 1
token_limit: 2000
# checkpoint_interval: 100000
save_base_dir: saved_models/


#query settings
query_min_length: 64
query_max_length: 128


data_path: data/ppo/

# Output generation
output_min_length: 64
output_max_length: 128
generation_kwargs:
  top_k: 20
  top_p: 0.9
  do_sample: true

//: TODO Store the mdoel in direcoty name gpt_ppo_90M/meta_data, model/ checkpoints_

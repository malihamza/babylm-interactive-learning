#!/bin/bash
#SBATCH -n 1
#SBATCH -c 8
#SBATCH --gres=gpu:A100:1
#SBATCH --constraint=80gb
#SBATCH --time=32:00:00
#SBATCH --job-name="PPO_train"
#SBATCH --output=logs/ppo/ppo_train_%j.log
#SBATCH --partition=scc-gpu
#SBATCH --mail-type=BEGIN,END,FAIL

set -euo pipefail

#--- Storage setup for SLURM script ---
# Ensure the job uses node-local SSD temp if available

if [ -n "$LOCAL_TMPDIR" ]; then
    export TMPDIR="$LOCAL_TMPDIR"
    echo "Using node-local SSD: $TMPDIR"
else
    echo "Using default TMPDIR: $TMPDIR"
fi

# ========== ENVIRONMENT SETUP ==========
if [[ -n "${SLURM_JOB_ID:-}" ]]; then
    module purge
    module load zstd/1.5.6 gcc/13.2.0 python/3.11.9 # cuda/12.6.2
    module load miniforge3
    # === Source conda ===
    # on SCC
    CONDA_EXE_PATH=$(which conda)
    CONDA_BASE=$(dirname $(dirname "$CONDA_EXE_PATH"))
    source "$CONDA_BASE/etc/profile.d/conda.sh"
    # on REACT
    #source $CONDA_PREFIX/etc/profile.d/conda.sh
    # === ===
    conda activate venv_hpc_blm
    echo "[INFO] Activated virtualenv at venv_hpc_blm"
else
    echo "[INFO] SLURM environment not detected. Skipping module/venv activation."
fi

# ========== Load HF and wandb credentials ==========
if [ -f ".env" ]; then
    source ".env"
fi
echo "Environment Check"
if [[ -z "${HF_TOKEN:-}" ]]; then
    warning "HF_TOKEN is not set! HuggingFace authentication will fail if accessing gated models."
fi
if [[ -z "${WANDB_API_KEY:-}" ]]; then
    warning "WANDB_API_KEY is not set! WandB logging may fail."
fi

# ========== RESOURCE LOGGING ==========

# Start GPU utilization logging
(
  echo "==== GPU Monitoring for job $SLURM_JOB_ID ===="
  while true; do
    date
    nvidia-smi --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.used,temperature.gpu --format=csv
    sleep 15
  done
) > "logs/ppo/gpu_usage_${SLURM_JOB_ID}.log" &
export GPU_LOG_PID=$!

# Start CPU + MEM logging
(
  echo "==== CPU+MEM Monitoring for job $SLURM_JOB_ID ===="
  while true; do
    date
    top -b -n 1 | head -20
    free -m
    sleep 15
  done
) > "logs/ppo/sys_usage_${SLURM_JOB_ID}.log" &
export SYS_LOG_PID=$!

# ========== RUN SCRIPT ==========
python -u ppo.py
echo "[INFO] PPO training complete."

cp -r $TMPDIR/saved_models ~/myprojects/BLM2025/saved_models/job_$SLURM_JOB_ID/

# ========== Cleanup logging jobs (safe even if already exited) ==========
if [[ -n "${GPU_LOG_PID:-}" ]]; then
    kill "$GPU_LOG_PID" 2>/dev/null || true
fi
if [[ -n "${SYS_LOG_PID:-}" ]]; then
    kill "$SYS_LOG_PID" 2>/dev/null || true
fi